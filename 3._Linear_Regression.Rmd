---
title: "Linear Regression"
output:
  word_document: default
  pdf_document: default
urlcolor: blue
---

# Short Definition

A linear regression is a statistical model that analyzes the relationship between a response variable (Y) and one or more input variables (X) that are also referred to as predictor or explanatory variables. We can use this model to predict Y when X is known (note that X is a vector of values (x1, x2, ... xn)). The mathematical equation can be generalized as follows:

Y = a + BX

, where *a* is known as the intercept, whereas *B* are coefficients of the predictor variables. Collectively, these are referred to as regression coefficients.

Linear regression assumes that there exists a linear relationship between the response variable Y and the explanatory variables X. In case of simple linear regression (only one input variable), this means that one can fit a line that approximates the relation between Y and X.

```{r knitr-logo, fig.align='center', echo=FALSE}
knitr::include_graphics(rep('images/linear_regression.png'))
```

# Boston housing dataset

Load the required R packages.

```{r message=FALSE}
# load MASS, corrplot and ggplot2
library(MASS)
#install.packages('corrplot')
library(corrplot)
library(ggplot2)
```

A package can include one or multiple datasets. We will use the *Boston* dataset available from the *MASS* package. Let's start by examining the dataset.

```{r}
# examine the structure of the Boston dataset
str(Boston)
```

```{r}
# check for the presence of NA values
apply(Boston, 2, function(x) sum(is.na(x)))
```

We will seek to predict *medv* variable (median house value) using (some of) the other 13 variables.

To find out more about the data set, type *?Boston*.

```{r}
# bring out the docs for the dataset 
?Boston
```

Let's start by examining which of the 13 predictors might be relevant for predicting the response variable (*medv*). One way to do that is to examine the correlation between the predictors and the response variable.

Since we have many variables, examining a correlation matrix will not be that easy. So, it is better to plot the correlations. To that end, we'll use the *corrplot* package (explore the plotting options offered by this package [here](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html)): 

```{r}
# compute the correlation matrix
corr.matrix <- cor(Boston)

# one option for plotting correlations: using colors to represent the extent of correlation
corrplot(corr.matrix, method = "number", type = "upper", diag = FALSE, number.cex=0.75, tl.cex = 0.85)

# another option, with both colors and exact correlation scores
corrplot.mixed(corr.matrix, tl.cex=0.75, number.cex=0.75)
```

Predictors *lstat* (percent of households with low socioeconomic status) and *rm* (average number of rooms per house) have the highest correlation with the outcome variable.

To examine this further, we can plot variables *lstat* and *rm* against the response variable.

```{r}
#  plot *lstat* against the response variable
ggplot(data = Boston, mapping = aes(x = lstat, y = medv)) +
  geom_point(shape = 1) +
  theme_classic()
```

```{r}
#  plot *rm* against the response variable
ggplot(data = Boston, mapping = aes(x = rm, y = medv)) +
  geom_point(shape = 1) +
  theme_classic()

```

# Simple Linear Regression

Let's start by building a simple linear regression model, with *medv* as the response and *lstat* as the predictor. We create this simple model just to showcase the basic functions related to linear regression. In the section *Multiple Linear Regression* we will create a more realistic model that includes multiple variables.

```{r}
# build an lm model with a formula: medv ~ lstat 
lm1 <- lm(medv ~ lstat, data = Boston)

# print the model summary
summary(lm1)
```

As we see, the *summary()* function gives us (among other things) the following:

* estimated values for the coefficients and their p-values; the latter indicate if a coefficent is significantly different than zero, that is, if the associated input variable is a significant predictor of the response variable  
* R-squared (R^2^) statistic,
* F-statistic for the model.

In particular, we can conclude the following:

* based on the coefficient of the *lstat* variable, with each unit increase in *lstat*, that is, with a percentage increase in the households with low socioeconomic status, median house value decreases by 0.95005 units; since outcome variable is expressed in 1000s USD, that further means that a percentage increase in the low-income households leads to a decrease of 950USD in the median house value 
* based on the R^2^ value, this model explains 54.4% of the variability in the median house value.
* based on the F statistic and the associated p-value, there is a significant relationship between the predictor and the response variable. Note that the F-test is a test of the overall significance of a regression model and it indicates whether the model provides a better fit to the data than a model with no independent variables.

To find out what other pieces of information are stored in the fitted model (that is, the *lm1* object), we can use the *names()* function.
```{r}
# print all attributes stored in the fitted model 
names(lm1)
```

So, for instance, to get the coefficients of the model:

```{r}
# print the coefficients
lm1$coefficients
```

Note, there is also the *coef()* function that returns the coefficients:

```{r}
# print the coefficients with the coef() f.
coef(lm1)
```

The **residual sum of squares (RSS)** is the sum of the squares of residuals (residuals are differences between the predicted and true values of the outcome variable). It measures the amount of variability that is left unexplained after performing the regression.

```{r}
# compute the RSS
lm1_rss <- sum(lm1$residuals^2)
lm1_rss
```

Recall that the obtained coefficient values are just estimates (of the real coefficient values) obtained using one particular sample from the target population. If some other sample was taken, these estimates might have been somewhat different. So, we usually compute the **95 confidence interval** for the coefficients to get an interval of values within which we can expect, in 95% of cases (i.e. 95% of examined samples), that the 'true' value for the coefficients will be.    

```{r}
# compute 95% confidence interval
confint(lm1, level = 0.95)
```

## Making predictions

Now that we have a model, we can predict the value of *medv* based on the given *lstat* values. To illustrate that, we will create a tiny test data frame.

```{r}
# create a test data frame containing only lstat variable and three observations: 5, 10, 15
df_test <- data.frame(lstat=c(5, 10, 15))

# calculate the predictions with the fitted model over the test data
predict(lm1, newdata = df_test)
```

We can also include the confidence interval for the predictions:

```{r}
# calculate the predictions with the fitted model over the test data, including the confidence interval
predict(lm1, newdata = df_test, interval = "confidence")
```

Or, we can examine prediction intervals:

```{r}
# calculate the predictions with the fitted model over the test data, including the prediction interval
predict(lm1, newdata = df_test, interval = "predict")
```

Notice the difference between the confidence and prediction intervals - the latter is much wider, reflecting far more uncertainty in the predicted value. This can be explained as follows: 

* **confidence intervals** capture uncertainty due to the error we make when estimating values of the regression coefficients (reducible error); 
* **prediction intervals** capture uncertainty that is due to both reducible error and irreducible error, the latter originating in the fact that our linear model is just an approximation of the true relationship between input and output variables (there is that random error in the model that we cannot avoid). 

To better understand the difference between the prediction and confidence intervals, you can check this ([YouTube video explanation](https://www.youtube.com/watch?v=o0UESA3UZss)).

To visually examine how well our model 'fits the data', we will first plot the data points for the *lstat* and *medv* variables. Next, we will add to the plot the regression line (blue coloured), with the confidence intervals (the gray area around the line).

```{r}
# plot the data points and the regression line
ggplot(data = Boston, mapping = aes(x = lstat, y = medv)) +
  geom_point(shape = 1) +
  geom_smooth(method = "lm")
```

The plot indicates that there is some non-linearity in the relationship between *lstat* and *medv*.

## Diagnostic Plots

Next, we will use *diagnostic plots* to examine how well our model satisfies the key assumptions of linear regression:

1. Linear relationship between the dependent and independent variables,
2. Normallity of residuals,
3. Homoscedasticity, meaning that the variance of residuals remains more-or-less constant across the regression line,
4. No high leverage points (observations that have unusually high / low predictor values are considered to have high leverage).

Four diagnostic plots are automatically produced by passing the output from *lm()* function (e.g. our *lm1*) to the *plot()* function. This will produce one plot at a time, and hitting *Enter* will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the *par()* function, which tells R to split the display screen into a grid of panels so that multiple plots can be viewed simultaneously.


```{r}
# split the plotting area into 4 cells
par(mfrow=c(2,2))

# print the diagnostic plots
plot(lm1)

# reset the plotting area
par(mfrow=c(1,1)) 
```

Interpretation of the plots:

* The 1st plot, **Residual vs Fitted value**, is used for checking if the linearity assumption is satisfied (Assumption 1). A pattern could show up in this plot if there is a non-linear relationship between the dependent and independent variables. If residuals are equally spread around a horizontal line without distinct patterns, that is an indication that there are no non-linear relationships. In this case, the plot indicates a non-linear relationship between the predictors and the response variable.
* The 2nd plot, **Normal Q-Q plot**, tells us if residuals are normally distributed (Assumption 2). The residuals should be lined well on the straight dashed line. In this case, we see a considerable deviation from the diagonal, and therefore, from the normal distribution.
* The 3rd plot, **Scale-Location**, is used for checking the assumption of the equal variance of residuals, homoscedasticity (Assumption 3). It’s good if there is a horizontal line with equally (randomly) spread points. In this case, the variance of the residuals tends to differ. So, the assumption is not fulfiled.
* The 4th plot, **Residuals vs Leverage**, is used for spotting the presence of high leverage points (Assumption 4). Their presence can seriously affect the estimation of the regression coefficients. They can be typically spotted in the corners, that is, beyond the dashed line that indicates the Cook’s distance (they have high Cook’s distance scores). In this case, there are several such observations.

For a nice explanation of the diagnostic plots, check the article [Understanding Diagnostic Plots for Linear Regression Analysis](http://data.library.virginia.edu/diagnostic-plots/).

If we want to examine leverage points in more detail, we can compute the leverage statistic using the *hatvalues()* function:
```{r}
# compute the leverage statistic
lm1.leverage <- hatvalues(lm1)
plot(lm1.leverage)
```

The plot suggests that there are several observations with high leverage values.

We can check this further by examining the value of leverage statistic for the observations. Leverage statistics is always between 1/n and 1 (n is the number of observations); observations with leverage statistic above 2*(p+1)/n (p is the number of predictors) are often considered as high leverage points.

Let's check this for our data:
```{r}
# calculate the number of high leverage points 
n <- nrow(Boston)
p <- 1
cutoff <- 2*(p+1)/n
length(which(lm1.leverage > cutoff))
```

The results confirm that there are several (34) high leverage points.

# Multiple Linear Regression

Let's now extend our model by including more predictor variables that have a high correlation with the response variable. Based on the correlation plot, we can include *rm* (average number of rooms per house) and *ptratio* (pupil-teacher ratio by town) variables into our model.

Scatterplot matrices are useful for examining the presence of a linear relationship between several pairs of variables.

```{r}
# generate the scatterplots for variables medv, lstat, rm, ptratio
pairs(~medv + lstat + rm + ptratio, data = Boston)
```

Far from proper linear relation, but let's see what the model will look like.

To be able to properly test our model (not use fictitious data points as we did in the case of simple linear regression), we need to split our dataset into:

1 **training data** that will be used to build a model,
2 **test data** to be used to evaluate/test the predictive power of our model.

Typically, 80% of observations are used for training and the rest for testing.

When splitting the dataset, we need to assure that observations are randomly assigned to the training and testing data sets. In addition, we should assure that the outcome variable has the same distribution in the train and test sets. This can be easily done using the *createDataPartition()* function from the *caret* package.

```{r message=FALSE}
# install.packages('caret')
library(caret)

# assure the replicability of the results by setting the seed 
set.seed(123)

# generate indices of the observations to be selected for the training set
train.indices <- createDataPartition(Boston$medv, p = 0.80, list = FALSE)
# select observations at the positions defined by the train.indices vector
train.boston <- Boston[train.indices,]
# select observations at the positions that are NOT in the train.indices vector
test.boston <- Boston[-train.indices,]
```

Check that the outcome variable (*medv*) has the same distribution in the training and test sets
```{r}
# print the summary of the outcome variable on both train and test sets
summary(train.boston$medv)
summary(test.boston$medv)
```

Now, build a model using the training data set.

```{r}
# build an lm model with a train dataset using the formula: medv ~ lstat + rm + ptratio
lm2 <- lm(medv ~ lstat + rm + ptratio, data = train.boston)

# print the model summary
summary(lm2)
```

From the summary, we can see that:

* R-squared has increased considerably, from `r round(summary(lm1)$r.squared, digits=3)` to 0.694 even though we have built it with a smaller dataset (`r nrow(train.boston)` observations, instead of `r n` observations),
* all 3 predictors are highly significant.

**TASK 1**: Interpret the estimated coefficients (see how it was done for the simple linear regression).

**TASK 2**: Use diagnostic plots to examine how well the model adheres to the assumptions. 

Let's make predictions using this model on the test data set that we have created.
```{r}
# calculate the predictions with the lm2 model over the test data
lm2.predict <- predict(lm2, newdata = test.boston)

# print out a few predictions
head(lm2.predict)
```

To examine the predicted against the real values of the response variable (*medv*), we can plot their distributions one against the other.

```{r}
# combine the test set with the predictions
test.boston.lm2 <- cbind(test.boston, pred = lm2.predict) 

# plot actual (medv) vs. predicted values
ggplot(data = test.boston.lm2) + 
  geom_density(mapping = aes(x=medv, color = 'real')) +
  geom_density(mapping = aes(x=pred, color = 'predicted')) +
  scale_colour_discrete(name ="medv distribution") + 
  theme_classic()
```

The distributions look similar, but we still fail to fully approximate the real values. 

To evaluate the predictive power of the model, we'll compute R-squared on the test data. Recall that R-squared is computed as $1-\frac{RSS}{TSS}$, where *TSS* is the total sum of squares.

```{r}
# calculate RSS
lm2.test.RSS <- sum((lm2.predict - test.boston$medv)^2)

# calculate TSS
lm.test.TSS <- sum((mean(train.boston$medv) - test.boston$medv)^2)

# calculate R-squared on the test data
lm2.test.R2 <- 1 - lm2.test.RSS/lm.test.TSS
lm2.test.R2
```

R^2^ on the test is slightly above the one obtained on the training set, which confirms the stability of our model.

Let's also compute **Root Mean Squared Error (RMSE)** to see how much error we are making with the predictions. Recall: $RMSE = \sqrt{\frac{RSS}{n}}$.
```{r}
# calculate RMSE
lm2.test.RMSE <- sqrt(lm2.test.RSS/nrow(test.boston))
lm2.test.RMSE
```

To get a perspective of how large this error is, let's check the mean value of the response variable on the test set.

```{r}
# compare medv mean to the RMSE
mean(test.boston$medv)
lm2.test.RMSE/mean(test.boston$medv)
```

So, it's not a small error, it's about 25% of the mean value.

Let's now build another model using all available predictors:
```{r}
# build an lm model with the training set using all of the variables
lm3 <- lm(medv ~ ., data = train.boston) # note the use of '.' to mean all variables

# print the model summary
summary(lm3)
```

Note that even though we now have 13 predictors, we haven't much improved the R^2^ value: in the model with 3 predictors (lm2), it was `r round(summary(lm2)$r.squared, digits=3)` and now it is `r round(summary(lm3)$r.squared, digits=3)`. In addition, it should be recalled that R^2^ increases with the increase in the number of predictors, no matter how good/useful they are.

The 3 predictors from the previous model are still highly significant. Plus, there are a number of other significant variables.

Considering the number of variables in the model, we should check for **multicolinearity** (one the assumptions of the multiple linear regression). 
*Collinearity* refers to a situation when 2 or more predictors are highly related to one another. A simple way to detect collinearity is to look at the correlation matrix of the predictors. However, it is possible for collinearity to exist between 3 or more variables even if no pair of variables has a particularly high correlation - this is known as *multicollinearity*.
One way to check for multicollinearity is to compute the **variance inflation factor (VIF)**:
```{r message=FALSE}
# load the 'car' package
library(car)
```
```{r}
# calculate vif
vif(lm3)
```

As a rule of thumb, variables having $\sqrt{vif} > 2$ are problematic.

```{r}
# calculate square root of the VIF and sort the computed values to facilitate inspection
sort(sqrt(vif(lm3)))
```

We can see that *tax* and *rad* variables exhibit multicollinearity. If we go back to the correlation plot, we'll see that they are, indeed, highly correlated (0.91). There are also a few other suspicious variables - *indus*, *nox*, and *dis* - since their VIF values are at the very threshold of what is considered acceptable.

So, we will create a new model (lm4) by excluding either *tax* or *rad* variable. Since *tax* has higher correlation with the outcome variable (than *rad*), we'll keep *tax* and exclude *rad*:
```{r}
# build an lm model with the training set using all of the variables except *rad*
lm4 <- lm(medv ~ . - rad, data = train.boston) # note the use of '-' to exclude the rad variable

# print the model summary
summary(lm4)
```

Let's do the prediction using the new model:
```{r}
# calculate the predictions with the lm3 model over the test data
lm4.predict <- predict(lm4, newdata = test.boston)
```

Plot the distribution of predictions against the real values of the response variable (*medv*).

```{r}
# combine the test set with the predictions
test.boston.lm4 <- cbind(test.boston, pred = lm4.predict) 

# plot actual (medv) vs. predicted values
ggplot(data = test.boston.lm4) + 
  geom_density(mapping = aes(x=medv, color = 'real')) +
  geom_density(mapping = aes(x=pred, color = 'predicted')) +
  scale_colour_discrete(name ="medv distribution") +
  theme_classic()
```

Based on the plot, the latest model seems to be a better fit for the data.

As before, we'll compute R^2^ on the test data.

```{r}
# calculate RSS
lm4.test.RSS <- sum((lm4.predict - test.boston$medv)^2)

# calculate R-squared on the test data
lm4.test.R2 <- 1 - lm4.test.RSS/lm.test.TSS
lm4.test.R2
```

Again, we got somewhat better R^2^ than on the train set.

We can also compute RMSE:

```{r}
# calculate RMSE
lm4.test.RMSE <- sqrt(lm4.test.RSS/nrow(test.boston))
lm4.test.RMSE
```

It is lower (therefore, better) than with the previous models.

**TASK**: use diagnostic plots to examine how well this model adheres to the assumptions. 